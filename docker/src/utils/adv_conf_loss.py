import json
import numpy as np
import matplotlib.pyplot as plt
import math
import scipy.stats as st
import matplotlib
#from fpdf import FPDF
import calendar
import time
import os

''' START CONFIG '''
# path of the json file containing the results generated by precision_loss.py
path_result = r"C:\Users\huebnean\OneDrive - Bertrandt AG\Dokumente\acl_results\yolop\final_results_acl_a2d2_yolop_nogt\result_detailed.json"

# choose the error bar to plot
PLOT_SD = False
PLOT_CI = True
PLOT_CUSTOM_ERROR_BAR = False

# if custom error bar is chosen you are able to configure the portion of values represented by the bar
CUSTOM_ERROR_BAR = 0.70

# set "True" if ACLs > 0 should be ignored (This values could already have been excluded when calculating the ACLs)
IGNORE_CONF_GAIN = True

# Choose which plots you want
PLOT_TRANSPARENT_DISTRIBUTION = True
PLOT_LINE_DISTRIBUTION = False
PLOT_CONFIDENCE_VALUES = True

# set "True" if PDF of results should be generated
GENERATE_RESULT_PDF = False

# PLOT_TRANSPARENT_DISTRIBUTION: configure color and other settings
REVERSED_COLORS = False
COLORMAP = plt.cm.OrRd
TRANSPARENT_COLOR = "green"
TEXT_NEXT_TO_MEAN = True

# PLOT_CONFIDENCE_VALUES: configure color settings
COLORMAP_CONF = plt.cm.BuGn
TRANSPARENT_COLOR_CONF = "red"

# if filters are not sorted by values, set "True"
SORT_FILTER = False

# set the step size for the interval to plot distribution
STEP_SIZE_DISTRIBUTION = 0.001

''' END CONFIG '''


''' START '''
def plot_acl_results(result_path, ignore_conf_gain, sort_filter, generate_pdf):
    '''
        This function plot the ACL results by using a json report file.

        :param result_path: path of the json report file
        :param ignore_conf_gain: TODO
        :param sort_filter: if true the values for a filter will be sorted by name
        :param generate_pdf: TODO
    '''

    # timestamp to store the files with unique names
    current_GMT = time.gmtime()
    timestamp = calendar.timegm(current_GMT)

    # get data from json file containing the results
    data = open(result_path)
    data_content = json.load(data)

    min_mean = None
    max_std = None

    step_distribution = 1 / STEP_SIZE_DISTRIBUTION

    # counter for unique image paths
    counter_for_plot_path = 0

    # get acl results
    acl_results = None
    if "step" in list(data_content.keys())[0]:
        for step, values in data_content.items():
            True #if "step_name" in values and values["step_name"] == "calculate_confidence_loss":
        acl_results = data_content["step0"]["step_0"]
    else:
        acl_results = data_content

    if not acl_results:
        print(f"No ACL results available in json file: {result_path}")
        return

    # loop over all acl results
    for filter, results_filter in acl_results.items():

        # variables start
        list_all_mean = []      # list with all mean ACL values for the current filter
        list_all_std = []       # list with all standard deviations for the current filter
        list_all_labels = []    # list with all labels for the current filter, sorted by means for plotting all values

        list_all_counter = []   # list with information about number of objects (confidence values) used to calculate each mean
                                # sorted by means for plotting the information next to each mean value

        list_all_cis = []               # list of all confidence intervals, sorted by means
        list_all_cis_lower_plot = []    # for each confidence interval lower end to plot is stored here
        list_all_cis_upper_plot = []    # for each confidence interval upper end to plot is stored here

        list_all_custom_error_bar_lower = []    # list of all lower custom error bar ends
        list_all_custom_error_bar_upper = []    # list of all upper custom error bar ends
        list_all_lower = []     # list of all values <= mean for each filter_value
        list_all_upper = []     # list of all values > mean for each filter_value

        list_conf_all = []      # list of all confidence values of the reference (original) object
        list_conf_mean = []     # list of all mean confidence values of the reference (original) object

        list_images_high_acl = []   # list of some image names with no detection
        list_images_low_acl = []    # list of some image names with detections similar to the original

        count_ACL_dict = {}    # dictionary with numbers of ACL values for each step of the interval [-1.00, 0.00] to plot the distribution
        # variables end

        # start reading and process information from result json file
        if isinstance(results_filter, dict) and filter not in ["Parameters", "Time"]:

            # sort the filter values if needed
            if sort_filter:
                results = dict(sorted(results_filter.items())).items()
            else:
                results = results_filter.items()

            # loop over all filter values of the current filter and their ACL results for all objects
            for filter_value, results_filter_value in results:

                # number of objects for each filter value
                object_count = 0

                # get all ACL results for current filter value
                conf_loss_list_dict = results_filter_value["conf_loss_list_dict"]

                # loop over each image containing ACL results for the current filter value
                for image_detail in conf_loss_list_dict.values():
                    # add number of objects that the current image contains
                    object_count = object_count + len(image_detail)

                # print information about filter name and value and number of images and objects
                print(f"{filter} {filter_value}: {len(conf_loss_list_dict.keys())} images")
                print(f"{filter} {filter_value}: {object_count} objects")
                print(f"")

                # add the same information to the a list for plotting them next to each filter value
                list_all_counter.append(#f"{filter} {filter_value}\n"
                                        f"{len(conf_loss_list_dict.keys())} images\n"
                                        f"{object_count} objects")

                # generate interval for plotting the distribution
                count_ACL_dict[filter_value] = {}
                count_list = []
                # TODO allow ignore_conf_gain = false
                for threshold in list(range(1, (int(step_distribution + 1)))):
                    #count_list.append(float(threshold / step_distribution)) # use if positive ACLs are also used
                    count_list.append(- float(threshold / step_distribution))
                    #threshold_pos = float(threshold / step_distribution) # use if positive ACLs are also used
                    threshold_neg = - float(threshold / step_distribution)
                    #count_ACL_dict[filter_value][threshold_pos] = 0 # use if positive ACLs are also used
                    count_ACL_dict[filter_value][threshold_neg] = 0
                count_list.sort()

                # check if mean ACL value is calculated for current filter value
                if results_filter_value["mean"] is not None:

                    # variables
                    conf_loss_list_upper = []   # list with all ACLs >= mean
                    conf_loss_list_lower = []   # list with all ACLs < mean
                    conf_list = []              # list with all confidence values of the reference (original) object

                    # get mean value and all results for current filter value
                    mean = results_filter_value["mean"]
                    conf_loss_dict_list = results_filter_value["conf_loss_list_dict"]

                    # generate unique label of filter name and current value
                    list_all_labels.append(filter + "_" + filter_value)

                    # add mean to list of all means
                    list_all_mean.append(mean)

                    # add std to list of all stds
                    list_all_std.append(results_filter_value["std_dvt"])

                    # count ACL values already used for custom error bar to stop if max number of values is reached
                    counter_for_custom_error_bar = 0

                    # loop over all images for the current filter value
                    for img_id, conf_loss_dict in conf_loss_dict_list.items():

                        # add number of ACLs to counter
                        counter_for_custom_error_bar += len(conf_loss_dict)

                        # loop over all objects / ACL values of the current image
                        for object_loss in conf_loss_dict:
                            # get ACL value and reference confidence value
                            conf_loss = object_loss["conf_loss_value"]
                            original_conf_value = object_loss["confidence_ref"]

                            # add the reference / original confidence value to a list
                            if original_conf_value:
                                conf_list.append(original_conf_value)
                            else:
                                conf_list.append(0)

                            # filter ACL values by higher or lower as mean
                            # and store some image names for high and low ACLs
                            if conf_loss >= mean:
                                conf_loss_list_upper.append(conf_loss)
                                if len(conf_loss_dict) == 1 and conf_loss > -0.01:
                                    list_images_low_acl.append({"image": img_id, "conf_loss": conf_loss, "filter": filter + "_" + filter_value})
                            else:
                                conf_loss_list_lower.append(conf_loss)
                                if len(conf_loss_dict) == 1 and conf_loss < -0.99:
                                    list_images_high_acl.append({"image": img_id, "conf_loss": conf_loss, "filter": filter + "_" + filter_value})

                            # get the interval of current ACL and store the value in the dictionary for plotting the distribution of all ACLs
                            if conf_loss < 0:
                                interval = math.floor((conf_loss) * step_distribution) / step_distribution
                                count_ACL_dict[filter_value][math.floor((conf_loss) * step_distribution) / step_distribution] += 1
                            #elif conf_loss > 0: # use if positive ACLs are also used
                                #count_ACL_dict[filter_value][math.ceil((conf_loss) * step_distribution) / step_distribution] += 1 # use if positive ACLs are also used
                            elif conf_loss == 0 and original_conf_value is not None:
                                count_ACL_dict[filter_value][-1 * STEP_SIZE_DISTRIBUTION] += 1


                    conf_loss_list_upper.sort(reverse = True)
                    conf_loss_list_lower.sort()

                    #conf_list.sort()
                    conf_mean = np.nanmean(conf_list)

                    count_all = 0
                    for _, count in count_ACL_dict[filter_value].items():
                        count_all += count

                    for interval, count in count_ACL_dict[filter_value].items():
                        count_ACL_dict[filter_value][interval] = count / count_all

                    list_all_lower.append(conf_loss_list_lower.copy())
                    list_all_upper.append(conf_loss_list_upper.copy())
                    list_conf_all.append(conf_list)
                    list_conf_mean.append(conf_mean)

                    stop_counting_values_for_error_bar = CUSTOM_ERROR_BAR * counter_for_custom_error_bar
                    try:
                        upper_error_bar = conf_loss_list_upper[0]
                    except:
                        upper_error_bar = mean

                    try:
                        lower_error_bar = conf_loss_list_lower[0]
                    except:
                        lower_error_bar = mean

                    # TODO: if IGNORE_CONF_GAIN is false
                    while counter_for_custom_error_bar > stop_counting_values_for_error_bar and PLOT_CUSTOM_ERROR_BAR:
                        if len(conf_loss_list_upper) != 0 and (len(conf_loss_list_lower) == 0 or abs(conf_loss_list_upper[0] - mean) >= abs(conf_loss_list_lower[0] - mean)):
                            upper_error_bar = conf_loss_list_upper[0]
                            conf_loss_list_upper.remove(upper_error_bar)
                        else:
                            lower_error_bar = conf_loss_list_lower[0]
                            conf_loss_list_lower.remove(lower_error_bar)

                        counter_for_custom_error_bar -= 1

                    list_all_custom_error_bar_lower.append(lower_error_bar - mean)
                    list_all_custom_error_bar_upper.append(mean - upper_error_bar)

                    if "ci" in results_filter_value:
                        list_all_cis.append(results_filter_value["ci"])

                        list_all_cis_upper_plot.append(results_filter_value["ci"][0] - mean)
                        list_all_cis_lower_plot.append(mean - results_filter_value["ci"][1])

                    else:
                        conf_loss_list = []
                        conf_loss_dict_list = results_filter_value["conf_loss_list_dict"]

                        for conf_loss_dict in conf_loss_dict_list:
                            for object_loss in conf_loss_dict["compared_objects"]:
                                conf_loss_list.append(object_loss["conf_loss_value"])

                        ci_conf_loss = st.norm.interval(alpha=0.99, loc=np.mean(conf_loss_list),
                                                        scale=st.sem(conf_loss_list))
                        list_all_cis.append(ci_conf_loss)

                        list_all_cis_upper_plot.append(ci_conf_loss[0] - mean)
                        list_all_cis_lower_plot.append(mean - ci_conf_loss[1])

            print("High ACL: " + str(list_images_high_acl))
            print("Low ACL: " + str(list_images_low_acl))



        if len(list_all_mean) > 0 and PLOT_TRANSPARENT_DISTRIBUTION:

            # size of figure
            matplotlib.rcParams.update({'font.size': 22})
            fig = plt.figure(figsize=(20, 14))
            ax1 = fig.add_subplot(111)

            colors = [COLORMAP(i) for i in np.linspace(0, 1, len(list_all_labels) * 2)]
            if REVERSED_COLORS:
                colors = list(reversed(colors))

            plt.axhline(0, color="black", linewidth=0.4)
            for i in np.arange(-0.9, -0, 0.1):
                plt.axhline(i, color="black", linewidth=0.3)
            plt.axhline(-1, color="black", linewidth=0.4)

            if min_mean is None and max_std is None:
                min_mean = min(list_all_mean)
                max_std = max(list_all_std)
            plt.ylim(-1, 0)
            # creating the bar plot
            font = {"fontsize": 22, "fontweight": "bold"}



            # plot distribution
            x = 0
            #for upper_list in list_all_upper:
                #for upper in upper_list:
                    #plt.plot(x, upper, marker="o", markersize=22, markeredgecolor=TRANSPARENT_COLOR, markerfacecolor=TRANSPARENT_COLOR, alpha=0.004,zorder=0)
                #x += 1

            #x = 0
            #for lower_list in list_all_lower:
                #for lower in lower_list:
                    #ax1.plot(x, lower, marker="o", markersize=22, markeredgecolor=TRANSPARENT_COLOR, markerfacecolor=TRANSPARENT_COLOR, alpha=0.004,zorder=0)
                #x += 1

            # x: list of the used filter, y: list of the mean for each color, yerr: list of the stds for each color to plot an error
            for i, label in enumerate(list_all_labels):
                mean_value = list_all_mean[i]
                ax1.errorbar([label], mean_value, yerr=[[list_all_custom_error_bar_upper[i]], [list_all_custom_error_bar_lower[i]]],
                             label=label, linewidth=6, capsize=0, fmt='o', color=colors[i + len(list_all_labels)], markersize=20, alpha=1, zorder=10)
            #plt.errorbar(list_all_labels, list_all_mean, yerr=list_all_std, capsize=4, fmt='o', color="#3C5A69", markersize=10)
            #plt.errorbar(list_all_labels, list_all_mean, capsize=8, fmt='o', color="#3C5A69", markersize=10)
            # some things to make the graphics pretty and understandable

            plt.setp(ax1.get_xticklabels(), rotation=15, ha='right')

            if TEXT_NEXT_TO_MEAN:
                x = 0
                for mean in list_all_mean:
                    ax1.text(x + 0.08, mean - 0.02, list_all_counter[x], fontsize=14)
                    x += 1

            ax1.xaxis.get_label().set_fontsize(18)
            # plt.xlabel("manipulations", fontdict=font)
            plt.ylabel("mean value of adversarial confidence loss", fontdict=font)
            plt.title(f"ACL: {filter}, Error bar: {CUSTOM_ERROR_BAR} of all values", fontdict=font)
            # save the plot as an image
            directory = os.path.dirname(result_path) + "/plots"
            if not os.path.exists(directory):
                os.makedirs(directory)
            counter_for_plot_path += 1
            plt.savefig(f"{directory}/plot_adv_conf_loss_{counter_for_plot_path}_{timestamp}.png")

            print(list_all_labels)
            print(list_all_mean)
            print(list_all_std)
            print(list_all_cis)
            print(list(zip(list_all_custom_error_bar_upper, list_all_custom_error_bar_lower)))

            print(f"End {filter}")

        if len(list_all_mean) > 0 and PLOT_LINE_DISTRIBUTION:
            counter = 0
            matplotlib.rcParams.update({'font.size': 25})
            fig = plt.figure(figsize=(35, 14))
            font = {"fontsize": 25, "fontweight": "bold"}
            ax1 = fig.add_subplot(111)

            colors = [COLORMAP(i) for i in np.linspace(0, 1, len(list_all_labels) * 2)]
            if REVERSED_COLORS:
                colors = list(reversed(colors))

            for filter_value, thresholds in count_ACL_dict.items():
                # create a list of all sorted object_ids for plotting the x-axis in the right order
                sorted_counts = []
                for threshold in count_list:
                    sorted_counts.append(thresholds[threshold] * 10 / STEP_SIZE_DISTRIBUTION)
                ax1.plot(count_list, sorted_counts, linewidth=14, alpha=0.7, label=filter_value)

            for i, j in enumerate(ax1.lines):
                j.set_color(colors[i + len(list_all_labels)])

            #ax1.legend()
            # some things to make the graphics pretty and understandable
            plt.xticks(rotation=90, fontsize=25)
            plt.yticks(fontsize=25)
            plt.xlabel("ACL values rounded to the nearest thousandth", fontdict=font)
            plt.ylabel("Distribution of ACL values in the entire dataset [%]", fontdict=font)
            plt.title(f"Distribution ACLs: {filter}", fontdict=font)
            # save the plot as an image
            directory = os.path.dirname(result_path) + "/plots"
            if not os.path.exists(directory):
                os.makedirs(directory)
            counter_for_plot_path += 1
            plt.savefig(f"{directory}/plot_adv_conf_loss_{counter_for_plot_path}_{timestamp}_distribution.png")

        if len(list_all_mean) > 0 and PLOT_CONFIDENCE_VALUES:

            # size of figure
            matplotlib.rcParams.update({'font.size': 22})
            fig = plt.figure(figsize=(20, 14))
            ax1 = fig.add_subplot(111)

            colors = [COLORMAP_CONF(i) for i in np.linspace(0, 1, len(list_all_labels) * 2)]
            if REVERSED_COLORS:
                colors = list(reversed(colors))

            plt.axhline(0, color="black", linewidth=0.4)
            for i in np.arange(0.1, 0.1, 0.9):
                plt.axhline(i, color="black", linewidth=0.3)
            plt.axhline(1, color="black", linewidth=0.4)
            plt.ylim(0, 1)
            # creating the bar plot
            font = {"fontsize": 22, "fontweight": "bold"}

            # plot distribution
            x = 0
            for list_conf in list_conf_all:
                for point in list_conf:
                    plt.plot(x, point, marker="o", markersize=22, markeredgecolor=TRANSPARENT_COLOR_CONF, markerfacecolor=TRANSPARENT_COLOR_CONF, alpha=0.004,zorder=0)
                x += 1

            # x: list of the used filter, y: list of the mean for each color, yerr: list of the stds for each color to plot an error
            for i, label in enumerate(list_all_labels):
                mean_value = list_conf_mean[i]
                if len(label.split("-")) > 1:
                    lab = label.split("-")[1]
                else:
                    lab = label.split("-")[0]
                plt.plot([label], mean_value, label=lab, marker="o", color=colors[i + len(list_all_labels)], markersize=20, alpha=1, zorder=10)

            plt.setp(ax1.get_xticklabels(), rotation=15, ha='right')

            if TEXT_NEXT_TO_MEAN:
                x = 0
                for mean in list_conf_mean:
                    ax1.text(x + 0.08, mean - 0.02, list_all_counter[x], fontsize=14)
                    x += 1

            ax1.xaxis.get_label().set_fontsize(18)
            # plt.xlabel("manipulations", fontdict=font)
            plt.ylabel("Confidence Score", fontdict=font)
            plt.title(f"Confidence Score: Distance", fontdict=font)
            # save the plot as an image
            directory = os.path.dirname(result_path) + "/plots"
            print("Result path: ", directory)
            if not os.path.exists(directory):
                os.makedirs(directory)
            counter_for_plot_path += 1
            plt.savefig(f"{directory}/plot_conf_score_{counter_for_plot_path}_{timestamp}.png")

    # TODO: Summary PDF
        '''
        # size of figure and font size
        fig = plt.figure(figsize = (90, 30))
        font = {"fontsize":35}
        #plt.ylim(0.5, 1)
        # set some things to make the graphics pretty and understandable
    
        plt.plot(list_manipulated_confidence, label=label, linewidth=2, color="green", alpha=0.6)
        plt.plot(list_original_confidence, label="original", linewidth=2, color="blue", alpha=0.6)
        # some things to make the graphics pretty and understandable
        plt.xticks(fontsize=25)
        plt.yticks(fontsize=25)
        plt.xlabel("Images", fontdict=font)
        plt.ylabel("Confidence Score", fontdict=font)
        plt.legend()
        ax = plt.gca()
        ax.axes.xaxis.set_ticklabels([])
        plt.title("Vergleich der Confidence-Werte f√ºr " + label + " und original", fontdict=font)
    
        # save the plot as an image
        plt.savefig(f"../results/plots/original_vs_{label}_{timestamp}.png")
    
    
        # size of figure
        fig = plt.figure(figsize=(20, 8))
        plt.ylim(-1.1, 0.15)
        plt.axhline(0, color="black", linewidth=0.3)
        # creating the bar plot
        font = {"fontsize": 12, "fontweight": "bold"}
        # x: list of the used filter, y: list of the mean for each color, yerr: list of the stds for each color to plot an error
        plt.errorbar(dict_color_mean.keys(), dict_color_mean.values(), yerr=dict_color_std.values(), capsize=4, fmt='o')
        # some things to make the graphics pretty and understandable
        plt.xticks(rotation=90)
        plt.xlabel("colors", fontdict=font)
        plt.ylabel("mean value of adversarial confidence loss", fontdict=font)
        plt.title("adversarial confidence loss for each color", fontdict=font)
        # save the plot as an image
        plt.savefig(f"../results/plots/original_vs_{label}_colors_{timestamp}.png")
        


        pdf = FPDF()
        pdf.add_page()
        pdf.set_xy(0, 0)
        pdf.set_font('arial', 'B', 12)
        pdf.cell(50)
        pdf.cell(90, 10, " ", 0, 2, 'C')
        pdf.cell(75, 10, "Adversarial Confidence Loss: Blur 8", 0, 2, 'C')
        pdf.cell(90, 10, " ", 0, 2, 'C')
        pdf.cell(-20)
        pdf.cell(50, 10, 'Color', 1, 0, 'C')
        pdf.cell(40, 10, 'Mean', 1, 0, 'C')
        pdf.cell(40, 10, 'Std', 1, 2, 'C')
        pdf.cell(-90)
        pdf.set_font('arial', '', 12)
        for i in range(0, len(list_all_labels)):
            pdf.cell(50, 10, '%s' % (list_all_labels[i]), 1, 0, 'C')
            pdf.cell(40, 10, '%s' % (str(round(list_all_mean[i], 3))), 1, 0, 'C')
            pdf.cell(-90)
        pdf.cell(90, 10, " ", 0, 2, 'C')
        pdf.cell(-30)
        directory = os.path.dirname(PATH_RESULTS) + "/plots"
        #try:
        #    pdf.image(f"{directory}/plot_adv_conf_loss_{counter_for_plot_path - 1}_{timestamp}.png", x=None, y=None, w=200, h=0, type='', link='')
        #except:
        #    continue
        pdf.add_page()
        pdf.cell(50)

        pdf.set_font('arial', 'B', 12)
        pdf.cell(75, 10, "Dataset Information", 0, 2, 'C')
        pdf.cell(90, 10, " ", 0, 2, 'C')
        pdf.cell(-40)
        pdf.set_font('arial', '', 10)
        pdf.cell(50, 10, 'Name', 1, 0, 'C')
        pdf.cell(100, 10, '%s' % info_content_manipulated["summary"]["dataset"]["dataset_name"], 1, 2, 'C')
        pdf.cell(-50)
        pdf.cell(50, 10, 'Size', 1, 0, 'C')
        pdf.cell(100, 10, '%s' % info_content_manipulated["summary"]["dataset"]["size"], 1, 2, 'C')
        pdf.cell(-50)
        pdf.cell(50, 10, 'Manipulations', 1, 0, 'C')
        pdf.cell(100, 10, '%s' % info_content_manipulated["summary"]["manipulations"], 1, 2, 'C')
        pdf.cell(-50)
        pdf.cell(50, 10, 'Original dataset', 1, 0, 'C')
        pdf.cell(100, 10, '%s' % info_content_manipulated["summary"]["original_dataset"]["dataset_name"], 1, 2, 'C')
    
        pdf.cell(90, 30, " ", 0, 2, 'C')
        pdf.cell(-20)
    
        pdf.set_font('arial', 'B', 12)
        pdf.cell(75, 10, "Inference Information", 0, 2, 'C')
        pdf.cell(90, 10, " ", 0, 2, 'C')
        pdf.cell(-30)
        
        pdf.set_font('arial', '', 10)
        pdf.cell(50, 10, 'Model', 1, 0, 'C')
        model_path = info_content_manipulated["inference"]["model"]["path"]
        pdf.cell(120, 10, '%s' % "model", 1, 2, 'C')
        pdf.cell(-50)
        pdf.cell(50, 10, 'Categories', 1, 0, 'C')
        pdf.cell(120, 10, '%s' % info_content_manipulated["inference"]["model"]["categories"], 1, 2, 'C')
        pdf.cell(-50)
        pdf.cell(50, 10, 'Input Size', 1, 0, 'C')
        pdf.cell(120, 10, '%s' % (str(info_content_manipulated["inference"]["model"]["input_size_w"]) + " x " + str(info_content_manipulated["inference"]["model"]["input_size_h"])), 1, 2, 'C')
        pdf.cell(-50)
        pdf.cell(50, 10, 'Min Confidence', 1, 0, 'C')
        pdf.cell(120, 10, '%s' % info_content_manipulated["inference"]["min_score_threshold"], 1, 2, 'C')
        pdf.cell(-50)
        pdf.cell(50, 10, 'IoU Threshold', 1, 0, 'C')
        pdf.cell(120, 10, '%s' % info_content_manipulated["inference"]["iou_threshold"], 1, 2, 'C')
        pdf.output(f"../results/plots/original_vs_{counter_for_plot_path}_colors_{timestamp}.pdf", 'F')
        '''


#plot_acl_results(path_result, IGNORE_CONF_GAIN, SORT_FILTER, GENERATE_RESULT_PDF)